## 1. 归档失败撑爆磁盘空间

### 环境信息

```
Red Hat Enterprise Linux (RHEL) 7.5
PostgreSQL 10.4
```

### 问题

```
归档失败撑爆磁盘空间导致数据库宕机
```

### 根本原因

```
pg_wal/archive_status准备归档的WAL文件已经不存在导致归档失败,WAL堆积至磁盘撑满,数据库宕机
```

### 处理步骤

> 由于线上业务,先手工处理堆积WAL日志

#### 1. 查看控制文件,找到哪些WAL文件是可以清理的

```
su - postgres

[postgres@tk226-nw-f09-x86-jdsjk-01 ~]$ pg_controldata 
pg_control version number:            1002
Catalog version number:               201707211
Database system identifier:           6494504333692061742
Database cluster state:               in production
pg_control last modified:             Mon 10 Dec 2018 11:09:32 PM CST
Latest checkpoint location:           1153/2E107278
Prior checkpoint location:            1149/67FFFEF8
Latest checkpoint's REDO location:    114E/327AFCA8
Latest checkpoint's REDO WAL file:    000000030000114900000067
```

#### 2. 调用`pg_archiveclean`清理WAL日志

```
pg_archivecleanup -d /flyingdata/FlyingDB/pgdata/pg_wal 000000030000114900000067
```

#### 3. 清理`archive_status`目录中相关文件

> 转移12月11号之前所有记录,使归档正常

```
cd $PGDATA/pg_wal/archive_status
ls --time-style=long -htl|grep ^- |awk 'BEGIN{date="2018-12-11"}{if($6<date)print $8}' |xargs -I {} mv {} /flyinglog/wal_archive/bak_archive_status/
```

### 参考

> <http://www.postgres.cn/docs/10/pgarchivecleanup.html>

## 2. 数据迁移引发的分区表序列id重复问题

### 环境信息

```
Red Hat Enterprise Linux (RHEL) 7.5
PostgreSQL 10.4
```

###  问题

```
在正式迁移的过程中，发现导入分区表xx的数据所产生的自增id（实际1700余万）和预估的id（预估是5800余万，原先5000余万,迁移导入800余万）对不上，整个分区表xx的id号发生了部分重复。
```

###  根本原因

```
数据库在正式迁移前做了一次迁移演练操作，使用pg_dump备份了正式库的表结构及数据(排除了分区表xx)，准备导入到已经存在分区表xx的新生产环境，验证下是否能够正常还原，经验证还原正常。但是此时的备份文件是包含分区表xx的序列的，此时序列被重置了，重置到了800余万。
```

###  诊断步骤

```
1、确认分区表的id存在重复。
	select id from xx group by id having count(id) > 1;
2、验证分区表多进程并发插入时，是否会存在取出的序列重复的情况。
	create table test(id int ,name varchar(20) not null);
	alter table test alter id set default nextval('m_id_seq');
	select create_hash_partitions('test','name',10);
	select set_enable_parent('test',false);
	...
	for ((i=1;i<100;i++))
	do
		nohup psql -c "insert into test(name) select left(md5(id::text),20) from generate_series(1,10000) t(id);" &
	done
	...
	select count(*) from test;
	select max(id) from test;
	select id from test group by id having count(id) > 1;
	经验证，不存在并发调用序列产生重复值的现象，所以id重复的问题肯定是人为造成的，和数据库分区表无关。
3、解决问题
	由于在迁移过程中，所以暂时先考虑如何解决这个问题。经过和项目经理沟通了解到这个id在表中仅作为一个唯一值来标记一条记录，可以重新产生新的序列id来重排记录，后续只要重新更新相关联的几张表的id键值即可。由于时间紧迫，从主表来导入导出数据明显耗时过长，考虑到分区表的特性，因此决定使用多进程直接从分区子表导出数据，清空表数据，然后在直接导入分区子表，完成数据的重排操作，最后更新相关表的id。
	
```

## 3. 连接池pgbouncer引起的性能问题

### 环境信息

```
Red Hat Enterprise Linux (RHEL) 7.5
PostgreSQL 10.4
```

### 问题

```
人脸系统压测时，tps波动非常大，并且呈现周期性的规律，而且tps比较之前的生产环境的上线前压测，压测结果较低。
```

### 根本原因

```
应用程序的抽取数据的程序并发量过大，且周期性的抽取数据，占用了连接的时间较长，引发了性能问题。
```

### 诊断步骤

```
1、人脸系统业务压测时，业务的tps的值较低，且周期性的存在低谷。客户提出数据库存在问题（当晚做了数据库参数调整），将参数改回去之后，再次压测，仍然出现上述问题。
2、和用户建议断开所有非压测程序的数据库访问连接时被拒绝了，客户的理由是测试的是当前生产环境下的tps值，断开应用程序的tps值不存在太大的参考价值，而且当前的tps完全不满足生产要求。
3、经过排查数据库日志，锁和服务器的负载以及pgbouncer的负载，未发现瓶颈问题，且tps下降呈现周期性的规律--每分钟都会产生峰谷，经过协调再次压测时，发现压力机的并发无论是高还是低都存在这种问题。
4、分析数据在整个应用系统链路上的节点，多次确认后，问题确实出现在后端数据库服务响应上，但是服务器负载和磁盘io上始终不存在什么压力，数据库日志里的耗时长的语句也并非压测产生。怀疑是连接池上存在一定问题。
5、开始分析pgbouncer连接池的日志，由于日志量过大，截取了压测时间内的日志去分析，发现压测时存在大量的pool full告警信息，于是推测，是不是某些应用程序存在大量周期性的并发访问，在访问期间会占用全部数据库连接（单个pgbouncer50个,两个pgbouncer100），导致连接池内产生排队队列，引起压测时的tps周期波动，并导致tps始终处于较低的状态。
6、提升连接池的默认并行数量限制，将default_pool_size由50改为100，之后由于时间的问题还未再进行压测，但是从单次服务响应时间上可以看到应用端记录的整个流程的执行时间上，数据库响应时间确实从秒级别提升到了几毫秒的时间。
7、和客户沟通后，确认有一个抽取数据的应用程序确实存在并发量过大的问题（5个进程，单进程50并发，总计并发访问量250），且24小时后台运行。
```