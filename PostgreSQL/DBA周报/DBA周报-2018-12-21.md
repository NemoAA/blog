## 1. 所有zk节点io负载大导致patroni降级主节点问题

### 环境信息

```
Red Hat Enterprise Linux Server release 7.4
PostgreSQL 10.1
```

### 问题

```
12月23日03:00:24时间，候补队列PG四个集群的所有在同一时间触发basebackup计划任务,四个主库（1,4,7,10节点）均发生重启。
```

### 根本原因

```
zk与数据库是安装在一起的，basebackup会使3个zk节点Io变得很大。
Zookeeper对于io的要求较为严格，当同一时间zk的所有节点（3,6,9节点）Io负载都比较高时。导致patroni在一段时间内连接不上zk。
当zk连接不上或者不健康时，patroni会在retry_timeout(默认10秒，设置为2秒)时间内重新去遍历连接zk的所有可用节点，只要能够连接上一个就会继续正常运作。如果这段时间内patroni依然不能够连接上zk。它会把主降级，避免发生脑裂的情况。当zk恢复后，再把主升级，恢复正常。
```

### 诊断步骤

```
1、查看4个集群状态，主节点均为发生切换，所有节点均正常运行。
2、查看数据库日志，发现四个主节点均在3点发生过重启。
3、查看ptroni日志，发现patroni有连接不上zk的告警，并且发现四个主节点均在3点发生过降级再升级的操作。
4、查看系统计划任务，发现在12台机器统一在每个星期天凌晨3点做基础备份。
5、查看zabbix，发现故障发生时，每台机器的io读写都很大，包括zk的三个节点。
6、在虚拟机上做测试，使用和线上生产环境一样的patroni故障切换配置，对数据库做大量插入操作，使磁盘io到峰值，完成故障重演。增大retry_timeout以及其他参数，patroni不会发生降级，再升级的操作。
7、在patroni主页上搜索相应的issues得到比较接近的案例：
https://github.com/zalando/patroni/issues/501
8、处理方案：
方案一：把3台有zk节点服务器上（3,6,9节点）的baseback任务计划岔开做，不让3个zk节点同时存在有IO压力。这样就算有一台zk不能提供服务，各个机器上patroni还可以连接其余两个节点。不会造成patroni连接不上zk的任何节点。
方案一存在的不足：
可以解决每个星期天备份导致主库重启的问题，但是当zk节点再次发生同时有大量io的情况，还会发生此类问题。由此，我们还有如下建议：

方案二：增大retry_timeout时间，让patroni尽量不要去降级主库。
方案二存在的取舍：
Patroni切换参数ttl,loop_wait,retry_timeout之间的设值一般规：
loop_wait+2*retry_timeout < ttl
当我们把retry_timeout设大后，需要把ttl参数也调大，以满足上的公式。
ttl的意义: 当一段时间内没有节点(主节点)更新dcs中leader key，则视作到期，删除并重新选举新的主节点，从而发生故障转移.
调大retry_timeout调大会增大patroni对故障的切换反应时间。

方案三：不要把zk节点与数据库节点放在一起，把zk节点搬到io不高且稳定的另外3台机器上去。这个方案是最优的解决方案。
```

## 2. 分区表数据的导入导出

### 环境信息

```
Red Hat Enterprise Linux Server release 7.4 (Maipo)
PostgreSQL 10.4, FlyingDB 3.4 和 PostgreSQL 11.1
```

### 问题

```
pg_dump数据导出时只能导入表结构并不会导入数据。
```

### 根本原因

```
使用pg_dump命令不仅需要父表，需要将分区表所有表全部选中。
更改前：pg_dump -h /home/pg11/ -d test -t test_hash -f table.sql
更改后：pg_dump -h /home/pg11/ -d test -t test_hash* -f table.sql
```

### 诊断步骤

#### 1. pg11分区表 单表导入导出测试

##### 1.1 创建测试分区表结构

```sql
test=# create table test_hash (id int,name varchar(10)) partition by hash (id);
test=# create table test_hash_0 partition of test_hash for values with(modulus 3,remainder 0);
test=# create table test_hash_1 partition of test_hash for values with(modulus 3,remainder 1);
test=# create table test_hash_2 partition of test_hash for values with(modulus 3,remainder 2);
```

##### 1.2 插入测试数据

```sql
test=# insert into test_hash select n,md5(n::text) from generate_series(0,10) n;
test=# select *,tableoid::regclass from test_hash;
 id |               name               |  tableoid   
----+----------------------------------+-------------
  2 | c81e728d9d4c2f636f067f89cc14862c | test_hash_0
  4 | a87ff679a2f3e71d9181a67b7542122c | test_hash_0
  6 | 1679091c5a880faf6fb5e6087eb1b2dc | test_hash_0
  8 | c9f0f895fb98ab9159f51fd0297e236d | test_hash_0
  3 | eccbc87e4b5ce2fe28308fd9f2a7baf3 | test_hash_1
  7 | 8f14e45fceea167a5a36dedd4bea2543 | test_hash_1
 10 | d3d9446802a44259755d38e6d163e820 | test_hash_1
  0 | cfcd208495d565ef66e7dff9f98764da | test_hash_2
  1 | c4ca4238a0b923820dcc509a6f75849b | test_hash_2
  5 | e4da3b7fbbce2345d7772b0674a318d5 | test_hash_2
  9 | 45c48cce2e2d7fbdea1afc51c7c6ad26 | test_hash_2
(11 rows)
```

##### 1.3 使用pg_dump工具导出表结构及数据

```sql
[pg11@test ~]$ pg_dump -h /home/pg11/ -d test -t test_hash* -f table.sql

## [pg11@test ~]$ cat table.sql 

## -- PostgreSQL database dump

-- Dumped from database version 11.1
-- Dumped by pg_dump version 11.1

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_with_oids = false;

--

## -- Name: test_hash; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.test_hash (
    id integer,
    name character varying
)
PARTITION BY HASH (id);

ALTER TABLE public.test_hash OWNER TO postgres;

--

## -- Name: test_hash_0; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.test_hash_0 PARTITION OF public.test_hash
FOR VALUES WITH (modulus 3, remainder 0);

ALTER TABLE public.test_hash_0 OWNER TO postgres;

--

## -- Name: test_hash_1; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.test_hash_1 PARTITION OF public.test_hash
FOR VALUES WITH (modulus 3, remainder 1);

ALTER TABLE public.test_hash_1 OWNER TO postgres;

--

## -- Name: test_hash_2; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.test_hash_2 PARTITION OF public.test_hash
FOR VALUES WITH (modulus 3, remainder 2);

ALTER TABLE public.test_hash_2 OWNER TO postgres;

--

## -- Data for Name: test_hash_0; Type: TABLE DATA; Schema: public; Owner: postgres

COPY public.test_hash_0 (id, name) FROM stdin;
2	c81e728d9d4c2f636f067f89cc14862c
4	a87ff679a2f3e71d9181a67b7542122c
6	1679091c5a880faf6fb5e6087eb1b2dc
8	c9f0f895fb98ab9159f51fd0297e236d
\.

--

## -- Data for Name: test_hash_1; Type: TABLE DATA; Schema: public; Owner: postgres

COPY public.test_hash_1 (id, name) FROM stdin;
3	eccbc87e4b5ce2fe28308fd9f2a7baf3
7	8f14e45fceea167a5a36dedd4bea2543
10	d3d9446802a44259755d38e6d163e820
\.

--

## -- Data for Name: test_hash_2; Type: TABLE DATA; Schema: public; Owner: postgres

COPY public.test_hash_2 (id, name) FROM stdin;
0	cfcd208495d565ef66e7dff9f98764da
1	c4ca4238a0b923820dcc509a6f75849b
5	e4da3b7fbbce2345d7772b0674a318d5
9	45c48cce2e2d7fbdea1afc51c7c6ad26
\.

--

## -- PostgreSQL database dump complete
```

##### 1.4 数据导入测试，并查看导入后的表结构及数据

```sql
[pg11@test ~]$ psql -h /home/pg11/ -f test.sql 
postgres=# \d+ test_hash
                                      Table "public.test_hash"
 Column |       Type        | Collation | Nullable | Default | Storage  | Stats target | Description 
--------+-------------------+-----------+----------+---------+----------+--------------+-------------
 id     | integer           |           |          |         | plain    |              | 
 name   | character varying |           |          |         | extended |              | 
Partition key: HASH (id)
Partitions: test_hash_0 FOR VALUES WITH (modulus 3, remainder 0),
            test_hash_1 FOR VALUES WITH (modulus 3, remainder 1),
            test_hash_2 FOR VALUES WITH (modulus 3, remainder 2)

postgres=# select *,tableoid::regclass from test_hash;
 id |               name               |  tableoid   
----+----------------------------------+-------------
  2 | c81e728d9d4c2f636f067f89cc14862c | test_hash_0
  4 | a87ff679a2f3e71d9181a67b7542122c | test_hash_0
  6 | 1679091c5a880faf6fb5e6087eb1b2dc | test_hash_0
  8 | c9f0f895fb98ab9159f51fd0297e236d | test_hash_0
  3 | eccbc87e4b5ce2fe28308fd9f2a7baf3 | test_hash_1
  7 | 8f14e45fceea167a5a36dedd4bea2543 | test_hash_1
 10 | d3d9446802a44259755d38e6d163e820 | test_hash_1
  0 | cfcd208495d565ef66e7dff9f98764da | test_hash_2
  1 | c4ca4238a0b923820dcc509a6f75849b | test_hash_2
  5 | e4da3b7fbbce2345d7772b0674a318d5 | test_hash_2
  9 | 45c48cce2e2d7fbdea1afc51c7c6ad26 | test_hash_2
(11 rows)
```

#### 2. 基于pg10的pathman分区表单表导入导出测试

##### 2.1 创建测试分区表结构

```sql
test=# create table test_hash (id int primary key,name varchar(10));
CREATE TABLE
test=# select create_hash_partitions('test_hash','id',3,false);         
 create_hash_partitions 
------------------------
                      3
(1 row)
```

##### 2.2 插入测试数据

```sql
test=# insert into test_hash select n,'abc'::text from generate_series(0,10) n;
INSERT 0 11
test=# select *,tableoid::regclass from test_hash;
 id | name |  tableoid   
----+------+-------------
  3 | abc  | test_hash_0
  5 | abc  | test_hash_0
  0 | abc  | test_hash_1
  2 | abc  | test_hash_1
  6 | abc  | test_hash_1
  7 | abc  | test_hash_1
  1 | abc  | test_hash_2
  4 | abc  | test_hash_2
  8 | abc  | test_hash_2
  9 | abc  | test_hash_2
 10 | abc  | test_hash_2
(11 rows)
```

##### 2.3 使用pg_dump工具导出表结构及数据

```sql
[postgres@test ~]$ pg_dump -d test -t test_hash* -f test.sql
pg_dump: WARNING:  COPY TO will only select rows from parent table "test_hash"
HINT:  Consider using the COPY (SELECT ...) TO variant.

[postgres@test ~]$ cat test.sql 

-- PostgreSQL database dump

-- Dumped from database version 10.4
-- Dumped by pg_dump version 10.4

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_with_oids = false;

--

## -- Name: test_hash; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.test_hash (
    id integer NOT NULL,
    name character varying(10)
);

ALTER TABLE public.test_hash OWNER TO postgres;

--

## -- Name: test_hash_0; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.test_hash_0 (
    CONSTRAINT pathman_test_hash_0_check CHECK ((public.get_hash_part_idx(hashint4(id), 3) = 0))
)
INHERITS (public.test_hash);

ALTER TABLE public.test_hash_0 OWNER TO postgres;

--

## -- Name: test_hash_1; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.test_hash_1 (
    CONSTRAINT pathman_test_hash_1_check CHECK ((public.get_hash_part_idx(hashint4(id), 3) = 1))
)
INHERITS (public.test_hash);

ALTER TABLE public.test_hash_1 OWNER TO postgres;

--

## -- Name: test_hash_2; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.test_hash_2 (
    CONSTRAINT pathman_test_hash_2_check CHECK ((public.get_hash_part_idx(hashint4(id), 3) = 2))
)
INHERITS (public.test_hash);

ALTER TABLE public.test_hash_2 OWNER TO postgres;

--

## -- Data for Name: test_hash; Type: TABLE DATA; Schema: public; Owner: postgres

COPY public.test_hash (id, name) FROM stdin;
\.

--

## -- Data for Name: test_hash_0; Type: TABLE DATA; Schema: public; Owner: postgres

COPY public.test_hash_0 (id, name) FROM stdin;
3	abc
5	abc
\.

--

## -- Data for Name: test_hash_1; Type: TABLE DATA; Schema: public; Owner: postgres

COPY public.test_hash_1 (id, name) FROM stdin;
0	abc
2	abc
6	abc
7	abc
\.

--

## -- Data for Name: test_hash_2; Type: TABLE DATA; Schema: public; Owner: postgres

COPY public.test_hash_2 (id, name) FROM stdin;
1	abc
4	abc
8	abc
9	abc
10	abc
\.

--

## -- Name: test_hash_0 test_hash_0_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.test_hash_0
    ADD CONSTRAINT test_hash_0_pkey PRIMARY KEY (id);

--

## -- Name: test_hash_1 test_hash_1_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.test_hash_1
    ADD CONSTRAINT test_hash_1_pkey PRIMARY KEY (id);

--

## -- Name: test_hash_2 test_hash_2_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.test_hash_2
    ADD CONSTRAINT test_hash_2_pkey PRIMARY KEY (id);

--

## -- Name: test_hash test_hash_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.test_hash
    ADD CONSTRAINT test_hash_pkey PRIMARY KEY (id);

--

## -- PostgreSQL database dump complete
```

##### 2.4 导入数据进行测试

```sql
[postgres@test ~]$ psql -d postgres -f test.sql 

postgres=# \d+ test_hash
                                        Table "public.test_hash"
 Column |         Type          | Collation | Nullable | Default | Storage  | Stats target | Description 
--------+-----------------------+-----------+----------+---------+----------+--------------+-------------
 id     | integer               |           | not null |         | plain    |              | 
 name   | character varying(10) |           |          |         | extended |              | 
Indexes:
    "test_hash_pkey" PRIMARY KEY, btree (id)
Child tables: test_hash_0,
              test_hash_1,
              test_hash_2

postgres=# select *,tableoid::regclass from test_hash;
 id | name |  tableoid   
----+------+-------------
  3 | abc  | test_hash_0
  5 | abc  | test_hash_0
  0 | abc  | test_hash_1
  2 | abc  | test_hash_1
  6 | abc  | test_hash_1
  7 | abc  | test_hash_1
  1 | abc  | test_hash_2
  4 | abc  | test_hash_2
  8 | abc  | test_hash_2
  9 | abc  | test_hash_2
 10 | abc  | test_hash_2
(11 rows)
```

#### 3. 用\copy命令导出分区表数据

##### 3.1 在pg11中测试

```sql
postgres=# \copy (select * from test_hash*) to table.csv with csv;
COPY 11

[pg11@test ~]$ cat table.csv 
2,c81e728d9d4c2f636f067f89cc14862c
4,a87ff679a2f3e71d9181a67b7542122c
6,1679091c5a880faf6fb5e6087eb1b2dc
8,c9f0f895fb98ab9159f51fd0297e236d
3,eccbc87e4b5ce2fe28308fd9f2a7baf3
7,8f14e45fceea167a5a36dedd4bea2543
10,d3d9446802a44259755d38e6d163e820
0,cfcd208495d565ef66e7dff9f98764da
1,c4ca4238a0b923820dcc509a6f75849b
5,e4da3b7fbbce2345d7772b0674a318d5
9,45c48cce2e2d7fbdea1afc51c7c6ad26

postgres=# truncate test_hash;
TRUNCATE TABLE
postgres=# \copy test_hash(id, name) from table.csv with csv;
COPY 11
postgres=# select *,tableoid::regclass from test_hash;
 id |               name               |  tableoid   
----+----------------------------------+-------------
  2 | c81e728d9d4c2f636f067f89cc14862c | test_hash_0
  4 | a87ff679a2f3e71d9181a67b7542122c | test_hash_0
  6 | 1679091c5a880faf6fb5e6087eb1b2dc | test_hash_0
  8 | c9f0f895fb98ab9159f51fd0297e236d | test_hash_0
  3 | eccbc87e4b5ce2fe28308fd9f2a7baf3 | test_hash_1
  7 | 8f14e45fceea167a5a36dedd4bea2543 | test_hash_1
 10 | d3d9446802a44259755d38e6d163e820 | test_hash_1
  0 | cfcd208495d565ef66e7dff9f98764da | test_hash_2
  1 | c4ca4238a0b923820dcc509a6f75849b | test_hash_2
  5 | e4da3b7fbbce2345d7772b0674a318d5 | test_hash_2
  9 | 45c48cce2e2d7fbdea1afc51c7c6ad26 | test_hash_2
(11 rows)
```

##### 3.2 在基于pg10的pathman分区表中测试

```sql
postgres=# \copy (select * from test_hash*) to table.csv with csv;
COPY 11

[postgres@test ~]$ cat table.csv 
0,abc
1,abc
2,abc
3,abc
4,abc
5,abc
6,abc
7,abc
8,abc
9,abc

postgres=# truncate test_hash;
TRUNCATE TABLE
postgres=# \copy test_hash(id,name) from table.csv with csv;
COPY 11
postgres=# select *,tableoid::regclass from test_hash;
 id | name | tableoid  
----+------+-----------
  0 | abc  | test_hash
  1 | abc  | test_hash
  2 | abc  | test_hash
  3 | abc  | test_hash
  4 | abc  | test_hash
  5 | abc  | test_hash
  6 | abc  | test_hash
  7 | abc  | test_hash
  8 | abc  | test_hash
  9 | abc  | test_hash
 10 | abc  | test_hash
(11 rows)
```

## 3. 由统计信息算法缺陷引起的性能故障

### 环境信息

```
Red Hat Enterprise Linux Server release 7.3 (Maipo)
FlyingDB 2.0.4
```

### 问题

```
用户反馈之前执行正常的查询，最近执行速度变慢了将近10倍。
```

### 根本原因

```
数据库的统计信息算法在对大表采样时有很小的几率会出现采样失误，导致产生的执行计划并不是实际上的最优计划
```

### 诊断步骤

```
1.查看问题SQL的执行计划如下：
catering=# explain analyze select r.company_id,c.company_abbreviation,c.company_name,sum(r.return_total) pdetail_total_price,r.record_type,a.pay_channel from ys10_cs_return r,sljy_merchant_company c,sljy_order_trade t,sljy_recrod_appendix a where a.pay_channel='J' and r.service_date='20181212' and c.company_id=r.company_id and r.record_type in ('B1','B2') and t.order_id=r.order_id and t.trade_no=a.trade_no and a.trade_type='1' group by r.company_id,c.company_name,c.company_abbreviation,r.record_type,a.pay_channel order by r.company_id;
                                                                                                QUERY PLAN                                                                 
                                
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------
 GroupAggregate  (cost=181085.58..181085.61 rows=1 width=77) (actual time=11886.956..11886.958 rows=2 loops=1)
   Group Key: r.company_id, c.company_name, c.company_abbreviation, r.record_type, a.pay_channel
   ->  Sort  (cost=181085.58..181085.58 rows=1 width=77) (actual time=11886.934..11886.934 rows=4 loops=1)
         Sort Key: r.company_id, c.company_name, c.company_abbreviation, r.record_type
         Sort Method: quicksort  Memory: 25kB
         ->  Nested Loop  (cost=0.99..181085.57 rows=1 width=77) (actual time=5041.842..11886.783 rows=4 loops=1)
               Join Filter: (r.company_id = c.company_id)
               Rows Removed by Join Filter: 3300
               ->  Nested Loop  (cost=0.99..181016.65 rows=1 width=21) (actual time=5009.650..11854.069 rows=4 loops=1)
                     Join Filter: ((t.trade_no)::text = (a.trade_no)::text)
                     Rows Removed by Join Filter: 24572
                     ->  Index Scan using sljy_recrod_appendix_order_code_pay_channel_idx on sljy_recrod_appendix a  (cost=0.43..81767.51 rows=1 width=30) (actual time=590
.646..1429.653 rows=24 loops=1)
                           Index Cond: ((pay_channel)::text = 'J'::text)
                           Filter: ((trade_type)::text = '1'::text)
                           Rows Removed by Filter: 2
                     ->  Nested Loop  (cost=0.56..99241.41 rows=618 width=47) (actual time=411.589..434.232 rows=1024 loops=24)
                           ->  Seq Scan on ys10_cs_return r  (cost=0.00..95873.01 rows=514 width=52) (actual time=411.558..426.869 rows=514 loops=24)
                                 Filter: (((record_type)::text = ANY ('{B1,B2}'::text[])) AND ((service_date)::text = '20181212'::text))
                                 Rows Removed by Filter: 731764
                           ->  Index Scan using sljy_order_trade_index on sljy_order_trade t  (cost=0.56..6.54 rows=1 width=61) (actual time=0.007..0.014 rows=2 loops=1233
6)
                                 Index Cond: ((order_id)::text = (r.order_id)::text)
               ->  Seq Scan on sljy_merchant_company c  (cost=0.00..59.52 rows=752 width=64) (actual time=0.902..8.106 rows=826 loops=4)
 Planning time: 201.833 ms
 Execution time: 11887.125 ms
(24 rows)

2.分析耗时阶段
	可以从执行计划里看到，当做join的时候选择了nestloop方式去执行join,这里的开销是比较大的，实际执行时间花了11854.069ms，几乎与执行时间耗时相等。
3.手动更改执行计划，强制走hash join
catering=# set enable_nestloop to off ;
SET
catering=# explain analyze select r.company_id,c.company_abbreviation,c.company_name,sum(r.return_total) pdetail_total_price,r.record_type,a.pay_channel from ys10_cs_return r,sljy_merchant_company c,sljy_order_trade t,sljy_recrod_appendix a where a.pay_channel='J' and r.service_date='20181212' and c.company_id=r.company_id and r.record_type in ('B1','B2') and t.order_id=r.order_id and t.trade_no=a.trade_no and a.trade_type='1' group by r.company_id,c.company_name,c.company_abbreviation,r.record_type,a.pay_channel order by r.company_id;
                                                                                                     QUERY PLAN                                                            
                                         
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------
 GroupAggregate  (cost=286353.55..286353.58 rows=1 width=77) (actual time=2084.577..2084.579 rows=2 loops=1)
   Group Key: r.company_id, c.company_name, c.company_abbreviation, r.record_type, a.pay_channel
   ->  Sort  (cost=286353.55..286353.56 rows=1 width=77) (actual time=2084.563..2084.564 rows=4 loops=1)
         Sort Key: r.company_id, c.company_name, c.company_abbreviation, r.record_type
         Sort Method: quicksort  Memory: 25kB
         ->  Hash Join  (cost=177715.88..286353.54 rows=1 width=77) (actual time=1988.515..2084.540 rows=4 loops=1)
               Hash Cond: (r.company_id = c.company_id)
               ->  Hash Join  (cost=177646.96..286284.61 rows=1 width=21) (actual time=1988.125..2084.145 rows=4 loops=1)
                     Hash Cond: ((t.order_id)::text = (r.order_id)::text)
                     ->  Hash Join  (cost=81767.52..190405.04 rows=1 width=35) (actual time=1527.370..1705.353 rows=24 loops=1)
                           Hash Cond: ((t.trade_no)::text = (a.trade_no)::text)
                           ->  Seq Scan on sljy_order_trade t  (cost=0.00..97880.46 rows=2868546 width=61) (actual time=0.004..1311.755 rows=2759843 loops=1)
                           ->  Hash  (cost=81767.51..81767.51 rows=1 width=30) (actual time=131.211..131.211 rows=24 loops=1)
                                 Buckets: 1024  Batches: 1  Memory Usage: 10kB
                                 ->  Index Scan using sljy_recrod_appendix_order_code_pay_channel_idx on sljy_recrod_appendix a  (cost=0.43..81767.51 rows=1 width=30) (act
ual time=2.820..131.187 rows=24 loops=1)
                                       Index Cond: ((pay_channel)::text = 'J'::text)
                                       Filter: ((trade_type)::text = '1'::text)
                                       Rows Removed by Filter: 2
                     ->  Hash  (cost=95873.01..95873.01 rows=514 width=52) (actual time=378.734..378.734 rows=514 loops=1)
                           Buckets: 1024  Batches: 1  Memory Usage: 51kB
                           ->  Seq Scan on ys10_cs_return r  (cost=0.00..95873.01 rows=514 width=52) (actual time=367.806..378.570 rows=514 loops=1)
                                 Filter: (((record_type)::text = ANY ('{B1,B2}'::text[])) AND ((service_date)::text = '20181212'::text))
                                 Rows Removed by Filter: 731764
               ->  Hash  (cost=59.52..59.52 rows=752 width=64) (actual time=0.380..0.380 rows=826 loops=1)
                     Buckets: 1024  Batches: 1  Memory Usage: 89kB
                     ->  Seq Scan on sljy_merchant_company c  (cost=0.00..59.52 rows=752 width=64) (actual time=0.006..0.187 rows=826 loops=1)
 Planning time: 2.321 ms
 Execution time: 2084.646 ms
(28 rows)
在这里可以看到执行计划更改之后，hash join的执行耗时1705.353，执行时间与用户反馈的之前执行时间差不多，所以怀疑是选择执行计划的时候出现了些变化，导致产生了一个非最优的执行计划。
4.分析执行计划产生的方式
postgresql数据库产生执行计划的时候是根据cost选择最优路径的，而hash Join和nestloop的计算方式如下：
hash join:C_outer + C_createhash + N_outer * C_hash
nestloop: C_outer + N_outer * C_inner
hash_join:对内表创建内存hash表的cost+扫描外表的cost+对外表每条记录做hash的cost
nestloop:扫描外表的cost+针对外表每条记录做N次循环判断的cost(N是内表的数量)
根据统计信息生成的执行计划来看，选择nestloop是没问题的，所以应该是统计信息出现了差错，从上面的执行计划可以看到系统预估的筛选出的内表记录数是1，而实际筛选出的结果是24，由此导致产生了不同的执行计划。
5.更新统计信息
当前统计信息如下：
catering=# select * from pg_statistic where starelid=18702 and staattnum=37;
-[ RECORD 1 ]--------------------
starelid    | 18702
staattnum   | 37
stainherit  | f
stanullfrac | 0
stawidth    | 2
stadistinct | 2
stakind1    | 1
stakind2    | 3
stakind3    | 0
stakind4    | 0
stakind5    | 0
staop1      | 98
staop2      | 664
staop3      | 0
staop4      | 0
staop5      | 0
stanumbers1 | {0.655633,0.344367}
stanumbers2 | {0.555161}
stanumbers3 | 
stanumbers4 | 
stanumbers5 | 
stavalues1  | {2,1}
stavalues2  | 
stavalues3  | 
stavalues4  | 
stavalues5  | 
从上面看到的是pay_channel这列采样数据里只包含了{1,2}两个值，而‘J‘并不在得到的取样值里，因此预估执行计划时把J的值估为了默认值1.这就导致优化器采用了nestloop的方式去做join.
analyze table；
执行后结果不变：
catering=# analyze sljy_recrod_appendix;
ANALYZE
catering=# select * from pg_statistic where starelid=18702 and staattnum=37;
-[ RECORD 1 ]----------------
starelid    | 18702
staattnum   | 37
stainherit  | f
stanullfrac | 0
stawidth    | 2
stadistinct | 2
stakind1    | 1
stakind2    | 3
stakind3    | 0
stakind4    | 0
stakind5    | 0
staop1      | 98
staop2      | 664
staop3      | 0
staop4      | 0
staop5      | 0
stanumbers1 | {0.6494,0.3506}
stanumbers2 | {0.546334}
stanumbers3 | 
stanumbers4 | 
stanumbers5 | 
stavalues1  | {2,1}
stavalues2  | 
stavalues3  | 
stavalues4  | 
stavalues5  | 
analyze采样的时候，由于系统默认采样数据量过小（default_statistics_target=100,采样数据300*100=30000条，相对于该表数据280W条，不足1%），导致未能准确采集到正确的数据，调整采样数据大小后再次更新统计信息（调整为1000,采样数据300000，采样比例略高于10%）。
catering=# alter table sljy_recrod_appendix alter COLUMN pay_channel set statistics 1000;
ALTER TABLE
catering=# analyze sljy_recrod_appendix;
ANALYZE
catering=# select * from pg_statistic where starelid=18702 and staattnum=37;
-[ RECORD 1 ]--------------------------------
starelid    | 18702
staattnum   | 37
stainherit  | f
stanullfrac | 0
stawidth    | 2
stadistinct | 3
stakind1    | 1
stakind2    | 3
stakind3    | 0
stakind4    | 0
stakind5    | 0
staop1      | 98
staop2      | 664
staop3      | 0
staop4      | 0
staop5      | 0
stanumbers1 | {0.653013,0.346973,1.33333e-05}
stanumbers2 | {0.544694}
stanumbers3 | 
stanumbers4 | 
stanumbers5 | 
stavalues1  | {2,1,J}
stavalues2  | 
stavalues3  | 
stavalues4  | 
stavalues5  | 

可以看到，这次统计信息更新后，采样的准确率与之前相比得到了提升。
6.再次执行查询
catering=# explain analyze select r.company_id,c.company_abbreviation,c.company_name,sum(r.return_total) pdetail_total_price,r.record_type,a.pay_channel from ys10_cs_return r,sljy_merchant_company c,sljy_order_trade t,sljy_recrod_appendix a where a.pay_channel='J' and r.service_date='20181212' and c.company_id=r.company_id and r.record_type in ('B1','B2') and t.order_id=r.order_id and t.trade_no=a.trade_no and a.trade_type='1' group by r.company_id,c.company_name,c.company_abbreviation,r.record_type,a.pay_channel order by r.company_id;
                                                                                                  QUERY PLAN                                                               
                                    
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------
 GroupAggregate  (cost=182532.37..182532.40 rows=1 width=77) (actual time=939.879..939.880 rows=2 loops=1)
   Group Key: r.company_id, c.company_name, c.company_abbreviation, r.record_type, a.pay_channel
   ->  Sort  (cost=182532.37..182532.37 rows=1 width=77) (actual time=939.865..939.867 rows=4 loops=1)
         Sort Key: r.company_id, c.company_name, c.company_abbreviation, r.record_type
         Sort Method: quicksort  Memory: 25kB
         ->  Nested Loop  (cost=0.99..182532.36 rows=1 width=77) (actual time=769.122..939.772 rows=4 loops=1)
               Join Filter: (r.company_id = c.company_id)
               Rows Removed by Join Filter: 3300
               ->  Nested Loop  (cost=0.99..182463.44 rows=1 width=21) (actual time=768.822..938.944 rows=4 loops=1)
                     Join Filter: ((t.trade_no)::text = (a.trade_no)::text)
                     Rows Removed by Join Filter: 24572
                     ->  Nested Loop  (cost=0.56..99241.41 rows=618 width=47) (actual time=665.564..708.866 rows=1024 loops=1)
                           ->  Seq Scan on ys10_cs_return r  (cost=0.00..95873.01 rows=514 width=52) (actual time=665.518..705.485 rows=514 loops=1)
                                 Filter: (((record_type)::text = ANY ('{B1,B2}'::text[])) AND ((service_date)::text = '20181212'::text))
                                 Rows Removed by Filter: 731764
                           ->  Index Scan using sljy_order_trade_index on sljy_order_trade t  (cost=0.56..6.54 rows=1 width=61) (actual time=0.004..0.006 rows=2 loops=514)
                                 Index Cond: ((order_id)::text = (r.order_id)::text)
                     ->  Materialize  (cost=0.43..82906.93 rows=34 width=30) (actual time=0.004..0.222 rows=24 loops=1024)
                           ->  Index Scan using sljy_recrod_appendix_order_code_pay_channel_idx on sljy_recrod_appendix a  (cost=0.43..82906.76 rows=34 width=30) (actual t
ime=4.529..225.927 rows=24 loops=1)
                                 Index Cond: ((pay_channel)::text = 'J'::text)
                                 Filter: ((trade_type)::text = '1'::text)
                                 Rows Removed by Filter: 2
               ->  Seq Scan on sljy_merchant_company c  (cost=0.00..59.52 rows=752 width=64) (actual time=0.004..0.131 rows=826 loops=4)
 Planning time: 4.913 ms
 Execution time: 940.022 ms
(25 rows)
可以看到，在物化时，根据统计信息预估的rows是34，与实际的24rows相差不大，而这个执行计划的耗时仅1s.
7.确认本次操作是否对其余查询产生影响
catering=# explain analyze select r.company_id,c.company_abbreviation,c.company_name,sum(r.return_total) pdetail_total_price,r.record_type,a.pay_channel from ys10_cs_return r,sljy_merchant_company c,sljy_order_trade t,sljy_recrod_appendix a where a.pay_channel='1' and r.service_date='20181212' and c.company_id=r.company_id and r.record_type in ('B1','B2') and t.order_id=r.order_id and t.trade_no=a.trade_no and a.trade_type='1' group by r.company_id,c.company_name,c.company_abbreviation,r.record_type,a.pay_channel order by r.company_id;
                                                                                    QUERY PLAN                                                                             
       
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------
 GroupAggregate  (cost=264381.28..264386.50 rows=190 width=77) (actual time=1959.952..1960.026 rows=33 loops=1)
   Group Key: r.company_id, c.company_name, c.company_abbreviation, r.record_type, a.pay_channel
   ->  Sort  (cost=264381.28..264381.75 rows=190 width=77) (actual time=1959.938..1959.956 rows=190 loops=1)
         Sort Key: r.company_id, c.company_name, c.company_abbreviation, r.record_type
         Sort Method: quicksort  Memory: 54kB
         ->  Hash Join  (cost=99318.06..264374.08 rows=190 width=77) (actual time=1647.182..1959.779 rows=190 loops=1)
               Hash Cond: (r.company_id = c.company_id)
               ->  Hash Join  (cost=99249.14..264302.55 rows=190 width=21) (actual time=1646.778..1959.276 rows=190 loops=1)
                     Hash Cond: ((a.trade_no)::text = (t.trade_no)::text)
                     ->  Seq Scan on sljy_recrod_appendix a  (cost=0.00..161751.49 rows=880006 width=30) (actual time=0.019..1457.444 rows=882674 loops=1)
                           Filter: (((pay_channel)::text = '1'::text) AND ((trade_type)::text = '1'::text))
                           Rows Removed by Filter: 1876692
                     ->  Hash  (cost=99241.41..99241.41 rows=618 width=47) (actual time=384.650..384.650 rows=1024 loops=1)
                           Buckets: 1024  Batches: 1  Memory Usage: 87kB
                           ->  Nested Loop  (cost=0.56..99241.41 rows=618 width=47) (actual time=371.034..384.371 rows=1024 loops=1)
                                 ->  Seq Scan on ys10_cs_return r  (cost=0.00..95873.01 rows=514 width=52) (actual time=370.999..382.119 rows=514 loops=1)
                                       Filter: (((record_type)::text = ANY ('{B1,B2}'::text[])) AND ((service_date)::text = '20181212'::text))
                                       Rows Removed by Filter: 731764
                                 ->  Index Scan using sljy_order_trade_index on sljy_order_trade t  (cost=0.56..6.54 rows=1 width=61) (actual time=0.003..0.004 rows=2 loop
s=514)
                                       Index Cond: ((order_id)::text = (r.order_id)::text)
               ->  Hash  (cost=59.52..59.52 rows=752 width=64) (actual time=0.391..0.391 rows=826 loops=1)
                     Buckets: 1024  Batches: 1  Memory Usage: 89kB
                     ->  Seq Scan on sljy_merchant_company c  (cost=0.00..59.52 rows=752 width=64) (actual time=0.006..0.191 rows=826 loops=1)
 Planning time: 4.962 ms
 Execution time: 1960.117 ms
(25 rows)
从执行计划和时间上看，没什么影响。
相对于之前的统计信息，本次操作所产生的新统计信息对1,2两个统计值的统计结果并未产生什么影响。

从效果上来看，本次调优的目的达到，且未对原有环境产生额外影响。

```